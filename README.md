# Hadoop Twitter Sentiment Analysis

This project demonstrates a comprehensive big data pipeline for performing sentiment analysis on Twitter data using Apache Hadoop and Spark. It covers data collection, cleaning, sentiment analysis, and visualization, showcasing a robust end-to-end solution for processing large-scale textual data.

## Features

*   **Twitter Data Collection:** Scrapes real-time tweets using the Tweepy library.
*   **Data Cleaning & Preprocessing:** Cleans raw tweet data, removing irrelevant information and preparing it for analysis.
*   **Distributed Sentiment Analysis:** Leverages Apache Spark (PySpark) to perform sentiment analysis on large datasets using both TextBlob and VADER sentiment analysis libraries.
*   **Text Feature Engineering:** Utilizes Spark MLlib for advanced text processing techniques like tokenization, stop word removal, and TF-IDF vectorization.
*   **Data Visualization:** Generates insightful visualizations, including sentiment distribution charts and word clouds for positive and negative tweets.
*   **Hadoop Integration:** Reads and writes processed data to Hadoop Distributed File System (HDFS), demonstrating big data ecosystem integration.

## Technologies & Tools

*   **Apache Spark (PySpark):** For distributed data processing and sentiment analysis.
*   **Apache Hadoop (HDFS):** For distributed storage of input and output data.
*   **Pandas:** For initial data cleaning and manipulation.
*   **Tweepy:** Python library for interacting with the Twitter API.
*   **TextBlob:** Python library for natural language processing, used for sentiment analysis.
*   **VADER Sentiment:** Rule-based sentiment analysis library, particularly effective for social media text.
*   **Matplotlib & Seaborn:** For creating static, interactive, and animated visualizations in Python.
*   **WordCloud:** For generating word cloud images from text data.

## Project Structure

*   `tweet_scraping.py`: Script to collect tweets using the Twitter API.
*   `clean_dataset.py`: Python script for cleaning and preprocessing the raw dataset.
*   `clean_dataset.ipynb`: Jupyter Notebook demonstrating the data cleaning process.
*   `pyspark_processing.py`: The core PySpark script for distributed sentiment analysis, text feature engineering, and visualization generation.
*   `customer_service_kaggle_dataset.csv`: Sample dataset used for demonstration.
*   `tweets.csv`: Cleaned dataset used as input for Spark processing.
*   `visualisations`: Directory containing generated sentiment distribution plots and word clouds.

## How to Run

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/your-username/HadoopTwitterSentiment.git
    cd HadoopTwitterSentiment
    ```
2.  **Set up Python Environment:**
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    pip install -r requirements.txt
    ```
3.  **Hadoop & Spark Setup:**
    *   Ensure you have Apache Hadoop and Apache Spark installed and configured on your system.
    *   Start your HDFS and YARN services.
    *   Create the necessary HDFS directory: `hdfs dfs -mkdir -p /twitter_data`
    *   Upload the `tweets.csv` (generated by `clean_dataset.py` or `clean_dataset.ipynb`) to HDFS: `hdfs dfs -put tweets.csv /twitter_data/`
4.  **Run Data Cleaning (Optional, if using `customer_service_kaggle_dataset.csv`):**
    ```bash
    python clean_dataset.py
    ```
5.  **Run PySpark Processing:**
    ```bash
    spark-submit pyspark_processing.py
    ```
    This script will perform the sentiment analysis, generate visualizations (saved to `visualisations/`), and save the output to HDFS (`/twitter_data/output_sentiments`).
6.  **View Visualizations:** Check the `visualisations/` directory for the generated plots.

## Visualizations

---
